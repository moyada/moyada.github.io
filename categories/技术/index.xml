<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>技术 | 不睡觉的猫头鹰</title>
    <link>https://moyada.github.io/categories/%E6%8A%80%E6%9C%AF/</link>
      <atom:link href="https://moyada.github.io/categories/%E6%8A%80%E6%9C%AF/index.xml" rel="self" type="application/rss+xml" />
    <description>技术</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>zh-Hans</language><copyright>© 2017 - 2020 moyada</copyright><lastBuildDate>Mon, 23 Mar 2020 00:28:42 +0800</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=true) shape:circle]</url>
      <title>技术</title>
      <link>https://moyada.github.io/categories/%E6%8A%80%E6%9C%AF/</link>
    </image>
    
    <item>
      <title>爬虫与反爬虫</title>
      <link>https://moyada.github.io/post/scrapy/</link>
      <pubDate>Mon, 23 Mar 2020 00:28:42 +0800</pubDate>
      <guid>https://moyada.github.io/post/scrapy/</guid>
      <description>&lt;p&gt;爬虫与反爬虫是个在软件开发行业里一直存在的斗争。爬虫，一般常见的有窃取同行数据、分析行业数据、恶意攻击等，最终都是以牟利为目的。为了防止爬虫，就诞生了反爬虫技术，通过识别非法操作与正常操作来做到拦截爬虫请求。&lt;/p&gt;
&lt;p&gt;下面就来分析一下这些常见的爬虫与反爬虫都有哪些手段。&lt;/p&gt;
&lt;h2 id=&#34;爬虫&#34;&gt;爬虫&lt;/h2&gt;
&lt;h3 id=&#34;api&#34;&gt;API&lt;/h3&gt;
&lt;p&gt;一般情况下，爬虫都是直接调用网站内请求数据的接口，这是成本最低的方式。通过分析接口请求规则，组装成 HTTP 请求，模拟正常用户操作。&lt;/p&gt;
&lt;p&gt;常见的 API 有&lt;code&gt;数据型&lt;/code&gt;和&lt;code&gt;操作型&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;例如通过某之家的接口 &lt;a href=&#34;https://car.m.autohome.com.cn/ashx/car/GetModelConfigNew3.ashx?seriesId=18&#34;&gt;https://car.m.autohome.com.cn/ashx/car/GetModelConfigNew3.ashx?seriesId=18&lt;/a&gt; ，通过接口可以获取到市场上车型参数的数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;而某东的登陆接口 &lt;a href=&#34;https://passport.jd.com/uc/loginService&#34;&gt;https://passport.jd.com/uc/loginService&lt;/a&gt; ，操作后保存 session，便可以在后面操作其他电商接口。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;通过这些接口的使用，就出现了数据分析工具，黄牛抢购，轰炸机脚本。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;模拟浏览器&#34;&gt;模拟浏览器&lt;/h3&gt;
&lt;p&gt;在一些网站对接口升级防护后，可能原有的方式就失效了，这个时候再尝试分析请求接口就可能徒劳无功。但是，通过模拟正常用户操作的话，那不是又能够越过防护了，这时模拟浏览器手段的爬虫开始流行起来。&lt;/p&gt;
&lt;p&gt;这类爬虫手段可以做到像手工操作一样模拟，登陆、验证码、获取网页元素，为了提高效率还可以设置禁止加载图片和 css。&lt;/p&gt;
&lt;p&gt;常用的工具有 &lt;code&gt;Python&lt;/code&gt; 的 
&lt;a href=&#34;https://www.selenium.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;selenium&lt;/a&gt; 和 
&lt;a href=&#34;https://miyakogi.github.io/pyppeteer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pyppeteer&lt;/a&gt;，两者都可以很方便的操作网页元素，前者是市面上相对成熟的爬虫工具，&lt;/p&gt;
&lt;h3 id=&#34;代理池&#34;&gt;代理池&lt;/h3&gt;
&lt;p&gt;上述的爬虫方式都是直接通过本机请求的，如果网站对来访的请求有流量控制策略，那么当请求过于频繁时，将可能被限制访问。&lt;/p&gt;
&lt;p&gt;这时可以通过 &lt;code&gt;匿名代理&lt;/code&gt; 的手段来隐藏真实 ip，通常是使用 ip 代理池来实现，这个代理池可以是自身维护的也可以是第三方的付费服务，每次请求都从这个 ip 代理池中获取 ip 代理操作，这样网站收集到的请求地址就是代理的 ip。&lt;/p&gt;
&lt;h2 id=&#34;反爬虫&#34;&gt;反爬虫&lt;/h2&gt;
&lt;h3 id=&#34;header-拦截&#34;&gt;Header 拦截&lt;/h3&gt;
&lt;p&gt;在网站的目标访问用户可预知的情况下，可以设置一些 Header、User-Agent、Cookie 来简单拦截非法请求。&lt;/p&gt;
&lt;p&gt;但是这种手段十分容易被攻破，只需通过抓取正常请求信息进行解析，再对爬虫请求配置请求头就可以模拟正常用户操作。&lt;/p&gt;
&lt;h3 id=&#34;验证码&#34;&gt;验证码&lt;/h3&gt;
&lt;p&gt;为了防止爬虫的大量请求影响网站可用性，通常会对一些关键接口增加验证码校验，比如字符验证码、问答验证码、滑动验证码、图形验证码。&lt;/p&gt;
&lt;p&gt;有些网站只是在接口外增加了验证码的保护，但爬虫不是像正常用户访问浏览器来请求的，而是绕过了验证码直接请求业务接口，所以需要将验证码与接口绑定在一起使用。&lt;/p&gt;
&lt;h3 id=&#34;数据加密&#34;&gt;数据加密&lt;/h3&gt;
&lt;p&gt;现在的网站为了防止爬虫，还会对页面数据进行混淆，直接返回 html 代码或使用 css 渲染数据，增加爬虫的成本。&lt;/p&gt;
&lt;p&gt;有的网站还会对一些关键接口的进行数据加密或者通信协议加密，需要动态跟踪加密过程来破解。这样使得爬虫分析接口变得几乎不可能，但是却避免不了通过模拟浏览器的操作。&lt;/p&gt;
&lt;h3 id=&#34;频次拦截&#34;&gt;频次拦截&lt;/h3&gt;
&lt;p&gt;在对爬虫敏感的网站，会对请求数据进行收集分析，通过识别爬虫行为来拦截请求，通常使用 &lt;code&gt;滑动窗口&lt;/code&gt; 对归类的请求进行统计。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ip 代理技术就是为了防止请求被拦截的匿名手段。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;通常情况下，为了做到高效的反爬虫，会将各个反爬虫手段结合使用，有些还会反向分析爬虫工具针对性拦截。&lt;/p&gt;
&lt;p&gt;然而，再完善的防御，还是会有被突破的可能，这一切都是成本的博弈。只有当爬虫行为的 ROI 远远低于预期时，那么反爬虫就是成功的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这是我的某宝下单脚本，由于该网站对交易请求做了加密，还在 selenium 做了很大功夫的防爬虫，于是便使用了能应对现有防御的 pyppeteer &lt;a href=&#34;https://github.com/moyada/auto-order&#34;&gt;https://github.com/moyada/auto-order&lt;/a&gt; 。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>第五届中间件性能挑战赛</title>
      <link>https://moyada.github.io/post/middleware-competition-5/</link>
      <pubDate>Sun, 08 Sep 2019 19:57:23 +0000</pubDate>
      <guid>https://moyada.github.io/post/middleware-competition-5/</guid>
      <description>&lt;p&gt;今年参与了阿里巴巴举办的
&lt;a href=&#34;https://tianchi.aliyun.com/competition/entrance/231714/introduction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;中间件性能挑战赛&lt;/a&gt;，并且有幸进入到了复赛，虽然后期由于其他原因没法专心参与，不过通过这次比赛也让我在技术上收获了不少，这边小小的做下本次比赛的总结。&lt;/p&gt;
&lt;h2 id=&#34;初赛自适应负载均衡的设计实现&#34;&gt;初赛：《自适应负载均衡的设计实现》&lt;/h2&gt;
&lt;h3 id=&#34;赛题分析&#34;&gt;赛题分析&lt;/h3&gt;
&lt;p&gt;初赛的题目是在 
&lt;a href=&#34;http://dubbo.apache.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dubbo&lt;/a&gt; 之上进行的插件开发，对参与者对于 Dubbo 的调用过程有一定的要求。官方在 gateway 端提供了 CallbackListenerImpl、TestClientFilter、UserLoadBalance 供开发。在 provider 端提供了 CallbackServiceImpl、TestRequestLimiter、TestServerFilter 供开发。&lt;/p&gt;
&lt;p&gt;将这些组件结合起来就形成了基本的调用链路。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://moyada.github.io/img/middleware-competition-5/adaptive-loadbalance.png&#34; alt=&#34;adaptive-loadbalance&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;目标&#34;&gt;目标&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;provider 在接收请求大于处理线程池大小的时候，将会执行拒绝策略 AbortPolicyWithReport，并将线程栈信息以日志文件输出。这无疑增加了服务端的压力，那么就需要让 provider 在 TestRequestLimiter 进行限流，在剩余线程数量不足时拒绝进来的请求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;要完成自适应的负载均衡，那么就需要由 provider 端将权重推送至 gateway，这项工作通过 CallbackServiceImpl 和 CallbackListenerImpl 来完成。但是传输的数据为字符串，不包含 provider 标识，这时就需要能够获取 provider 的服务地址，连带指标推送给 gateway。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;gateway 接收到请求时选择性能最优 provider，当所有 provider 压力均满时直接拒绝请求。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;设计与实现&#34;&gt;设计与实现&lt;/h3&gt;
&lt;h4 id=&#34;provider-拒绝服务&#34;&gt;provider 拒绝服务&lt;/h4&gt;
&lt;p&gt;通过分析 &lt;code&gt;AllChannelHandler&lt;/code&gt; 的源码发现，provider 在通过 Filter 后才会使用线程池处理请求，为了不让服务进入拒绝策略，就需要在线程池不足时利用 Filter 拒绝他。官方是允许使用 SPI 的，那么可根据 &lt;code&gt;WrappedChannelHandler&lt;/code&gt; 的代码，从 &lt;code&gt;DataStore&lt;/code&gt; 里获取所使用的线程池，提取出线程池最大线程数，在请求前后进行计数，拦截超出能力的请求。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;public class WrappedChannelHandler implements ChannelHandlerDelegate {

    public WrappedChannelHandler(ChannelHandler handler, URL url) {
        this.handler = handler;
        this.url = url;
        executor = (ExecutorService) ExtensionLoader.getExtensionLoader(ThreadPool.class)
        		.getAdaptiveExtension().getExecutor(url);

        String componentKey = Constants.EXECUTOR_SERVICE_COMPONENT_KEY;
        if (Constants.CONSUMER_SIDE.equalsIgnoreCase(url.getParameter(Constants.SIDE_KEY))) {
            componentKey = Constants.CONSUMER_SIDE;
        }
        DataStore dataStore = ExtensionLoader.getExtensionLoader(DataStore.class).getDefaultExtension();
        dataStore.put(componentKey, Integer.toString(url.getPort()), executor);
    }

...
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;经过了这一层拦截，分数就已经明显上去了，但是由于 provider 的能力各不相同，还需要通过计算权重进行请求分发。&lt;/p&gt;
&lt;h4 id=&#34;数据通信&#34;&gt;数据通信&lt;/h4&gt;
&lt;p&gt;当消费方与多个服务方都需要进行通信时，能够唯一确定数据的来源是关键，还是通过读源码，我发现能够从服务 URL 中获取到主机地址和端口号。在 provider 端，自定义暴露监听器保存系统的地址。在 gateway 端，自定义引用监听器将 URL 与地址进行一一映射。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// gateway

public class InvokerMapperListener implements InvokerListener {

private static final Map&amp;lt;URL, String&amp;gt; addressMapper = new ConcurrentHashMap&amp;lt;&amp;gt;();

    @Override
    public void referred(Invoker&amp;lt;?&amp;gt; invoker) throws RpcException {
        URL url = invoker.getUrl();
        String address = getAddress(url);

	addressMapper.put(url, address);
    }
}

// provider

public class ThreadPoolExporterListener implements ExporterListener {

    @Override
    public void exported(Exporter&amp;lt;?&amp;gt; exporter) throws RpcException {
	if (Context.ADDRESS != null) {
		return;
	}

	URL url = exporter.getInvoker().getUrl();
        Context.ADDRESS = getAddress(url);
    }
}

public static String getAddress(URL url) {
    String ip = url.getParameter(Constants.BIND_IP_KEY);
    String port = url.getParameter(Constants.BIND_PORT_KEY);
    return ip + &amp;quot;:&amp;quot; + port;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通信功能算是完成了，这时可以直接使用前面 provider 获取到的线程池大小作为权重，但是由于服务能力是动态变化的，为了达到更好的效果还需要进一步改进。&lt;/p&gt;
&lt;h4 id=&#34;动态负载&#34;&gt;动态负载&lt;/h4&gt;
&lt;p&gt;根据评测代码可知，消费服务在程序内部使用了 &lt;code&gt;Semaphore&lt;/code&gt; 对请求进行了拦截，实际可容纳的请求小于线程数，并且并发数随着时间也在发生变化。&lt;/p&gt;
&lt;p&gt;如果单位时间内接收的请求数大于实际并发数时，则多余的请求将会被置为等待状态。当请求数持续大于并发数时，会导致请求堆积造成大量超时。因此在 Dubbo 中有个超时检测机制 &lt;code&gt;org.apache.dubbo.remoting.exchange.support.DefaultFuture##TIME_OUT_TIMER&lt;/code&gt;，这是一个用于中止任务的时间轮，当一个请求进入执行阶段时，它将会被放入时间轮，在一定时间后结束请求返回错误信息。&lt;/p&gt;
&lt;p&gt;因此需使得请求在时间轮触发之前执行，这样才能保证不会造成异常响应。在这里我还是使用到了前面获取的线程池，由于线程在线程池中空闲的状态为 &lt;code&gt;WAITING&lt;/code&gt; 或 &lt;code&gt;TIMED_WAITING&lt;/code&gt;，在执行过程中工作线程的状态可能为 &lt;code&gt;RUNNABLE&lt;/code&gt;、&lt;code&gt;BLOCKED&lt;/code&gt;、&lt;code&gt;WAITING&lt;/code&gt;、&lt;code&gt;TIMED_WAITING&lt;/code&gt;，空闲线程与工作线程存在状态重叠，因而无法将 &lt;code&gt;getActiveCount&lt;/code&gt; 用作并发数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://moyada.github.io/img/middleware-competition-5/thread_state.png&#34; alt=&#34;thread_state&#34;&gt;&lt;/p&gt;
&lt;p&gt;因此在执行阶段 Filter 前后对当前线程进行标记，并放入一个 placeHolder 中保存，在退出 Filter 时移除标记。由此一来 placeHolder 中就只存在工作中的线程，在除去 &lt;code&gt;BLOCKED&lt;/code&gt; 和 &lt;code&gt;WAITING&lt;/code&gt; 状态的线程即为并发数。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;为了避免并发冲突影响效率，采用无锁编程，预先创建好数组，利用线程id作为下标指定标记元素位置。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在得到实际并发数后，再通过与 cpu核心数、cpu频率 进行计算得出权重值，定时的发送给 gateway 用于选择最优 provider。&lt;/p&gt;
&lt;p&gt;gateway 将收到并发数作为负载均衡还远远不够，如果一个服务由于机器资源不足、网络故障等原因也将会导致请求异常，因此请求耗时也是一个考量负载的指标。由于网络消耗的存在，则在 gateway 端做请求耗时的收集，将并发数与耗时进行结合完成负载均衡策略。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://moyada.github.io/img/middleware-competition-5/adaptive-loadbalance-architecture.png&#34; alt=&#34;adaptive-loadbalance-architecture&#34; title=&#34;最终架构图&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;复赛实现一个进程内基于队列的消息持久化存储引擎&#34;&gt;复赛：《实现一个进程内基于队列的消息持久化存储引擎》&lt;/h2&gt;
&lt;h3 id=&#34;赛题分析-1&#34;&gt;赛题分析&lt;/h3&gt;
&lt;p&gt;赛题要求实现一个进程内消息持久化的存储引擎，提供了 4g 的堆内存，2g 的堆外内存，300g 的SSD磁盘。过程分别为发送、查询聚合消息、查询聚合结果，串行执行三个步骤，每个步骤需要在 30 分钟内完成，以每个阶段的 发送数\时间 总和为成绩。&lt;/p&gt;
&lt;h3 id=&#34;设计&#34;&gt;设计&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;由于查询阶段需要对 时间戳 和 数值 进行条件过滤，考虑以 时间戳 和 数值 创建索引键对数据分散进行存储，以数据块为存储单位。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个线程所发送消息的时间戳是递增的，对查询过滤天然适合，则为每个线程映射一个存储组件实例。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;官方提供使用的存储介质为 SSD磁盘，大量数据写入的效率好，所以将数个数据块为一组单位，缓存于内存中，在写入数据达到下一组单位时进行文件存储。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在查询阶段为了更好的利用资源，使用 lru缓存 保存查询数据。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://moyada.github.io/img/middleware-competition-5/data_write.png&#34; alt=&#34;data_write&#34; title=&#34;存储组件&#34;&gt;&lt;/p&gt;
&lt;p&gt;发送阶段完成后磁盘文件组织如下，以时间戳定位文件，再以数值选择文件内的数据块（图中以 8192 为时间戳分片大小，4096 为数值为片大小，均为 2 的幂次方，以位运算即可完成路由）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://moyada.github.io/img/middleware-competition-5/file_store.png&#34; alt=&#34;file_store&#34; title=&#34;文件编排&#34;&gt;&lt;/p&gt;
&lt;p&gt;如查询条件为 &lt;code&gt;{ t: [8000, 22000], a: [5000, 20000] }&lt;/code&gt;，则选择文件 hash-0、hash-1、hash-2 为目标文件，分别对文件内的 index-1、index-2、index-3、index-4 数据块进行检索，其中 &lt;code&gt;hash-1 index-2&lt;/code&gt;、&lt;code&gt;hash-1 index-3&lt;/code&gt; 为中间数据，无需对数据进行条件过滤。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://moyada.github.io/img/middleware-competition-5/data_read.png&#34; alt=&#34;data_read&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;实现&#34;&gt;实现&lt;/h3&gt;
&lt;p&gt;其实复赛的设计思路并不难，主要的难点是分片大小的选择，如何更好的利用内存不发生 OOM，以及利用多线程提升效率。&lt;/p&gt;
&lt;p&gt;在写入阶段，使用内存进行数据的缓存，利用时间戳的有序性检测是否进入下个文件数据集，由于需要对全量数据都进行哈希，采用了位移操作提升效率。&lt;/p&gt;
&lt;p&gt;在读取阶段，对条件使用哈希确定目标数据的分布范围，充分利用堆内存与堆外内存，将分配的 &lt;code&gt;ByteBuffer&lt;/code&gt; 进行缓存，提高下次查询时的效率。同理，对于无需条件过滤的结果也可以通过缓存来提高效率。&lt;/p&gt;
&lt;p&gt;增加了缓存就需要考虑内存容量问题，为了避免 OOM 需要引入淘汰规则。直接使用 &lt;code&gt;SoftReference&lt;/code&gt; 容易频繁产生 CMS GC，影响查询效率，而使用 &lt;code&gt;WeakReference&lt;/code&gt; 缓存数据则具有不确定性，数据很可能被快速回收，失去了缓存的意义。&lt;/p&gt;
&lt;p&gt;经过分析后选择了LRU算法，在固定大小集合里缓存数据，对长久未使用的数据进行清理。传统的 LRU Cache 可用继承 &lt;code&gt;LinkedHashMap&lt;/code&gt; 的方式实现，然而这个类是非线程安全的，在查询场景可能产生并发问题，因此重新实现了一个线程安全的 LRU Cache。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;public class ConcurrentLRUCache&amp;lt;K, V&amp;gt; {

    private final Queue&amp;lt;K&amp;gt; keySet;
    private final Map&amp;lt;K, T&amp;gt; cache;

    private final AtomicInteger acquire;
    private final int outSize;

    private volatile boolean full = false;

    public ConcurrentLRUCache(int size) {
        this.keySet = new ConcurrentLinkedQueue&amp;lt;&amp;gt;();
        this.cache = new ConcurrentHashMap&amp;lt;&amp;gt;(size, 1f);
        this.acquire = new AtomicInteger(size);
        this.outSize = size + 1;
    }

    public void put(K key, T value) {
        if (full) {
            K firstKey = keySet.poll();
            cache.remove(firstKey);
        } else {
            int nextSize = acquire.incrementAndGet();

            if (nextSize &amp;gt; outSize) {
                Thread.yield();
                put(key, value);
                return;
            }

            if (nextSize == outSize) {
                full = true;
            }
        }
        cache.put(key, value);
        keySet.offer(key);
    }

    public T get(K key) {
        T value = cache.get(key);
        if (value == null) {
            return null;
        }
        
        if (keySet.remove(key)) {
            keySet.offer(key);
        }
        return value;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;除了添加缓存外，还有其他优化手段，比如 多线程并发处理数据块、对分块数据进行压缩、数据与消息体分开存储、利用时间戳的顺序中断查询。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;在这次比赛中，主要收获了在系统设计上的锻炼，分析问题也更加全面了，以及在 &lt;code&gt;无锁编程&lt;/code&gt;、&lt;code&gt;NIO处理&lt;/code&gt;、&lt;code&gt;编写高性能代码&lt;/code&gt; 方面的进步。如通过 &lt;code&gt;async-profiler&lt;/code&gt; 发现乐观锁在大量请求下的性能问题，并转变思路使用无锁编程。了解到 &lt;code&gt;MappedByteBuffer&lt;/code&gt; 在大量写入的情况下会频繁发生 pageCache 的 &lt;code&gt;flush&lt;/code&gt;，可以先准备好 &lt;code&gt;ByteBuffer&lt;/code&gt; 保存数据，之后再将整个 &lt;code&gt;ByteBuffer&lt;/code&gt; 写入到 &lt;code&gt;MappedByteBuffer&lt;/code&gt; 中。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用 Prometheus &#43; Grafana 搭建监控系统</title>
      <link>https://moyada.github.io/post/prometheus-grafana/</link>
      <pubDate>Sat, 23 Mar 2019 15:18:30 +0000</pubDate>
      <guid>https://moyada.github.io/post/prometheus-grafana/</guid>
      <description>&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;系统监控是中大型企业中必不可少的组件，在线上服务器遇到负载问题时通过监控便能够查看系统状态快照，还能够通过监控系统开发其他效能组件，进一步提高系统的可用性。&lt;/p&gt;
&lt;h2 id=&#34;监控应用程序&#34;&gt;监控应用程序&lt;/h2&gt;
&lt;p&gt;通常情况下，要想获得应用的负载情况就需要改造原有工程，提供 pull 或 push 的方式将应用状况进行上报。&lt;/p&gt;
&lt;p&gt;Prometheus 使用的是 pull 模式，在应用中开放端口提供系统状态信息，监控系统可以定时从应用中拉取记录。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在 SpringBoot 应用中引入依赖&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- prometheus --&amp;gt;
&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;io.micrometer&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;micrometer-registry-prometheus&amp;lt;/artifactId&amp;gt;
&amp;lt;/dependency&amp;gt;

&amp;lt;!-- endpoint --&amp;gt;
&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spring-boot-starter-actuator&amp;lt;/artifactId&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;配置暴露的 endpoint 及系统标识&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;management.metrics.tags.application=${spring.application.name}
# 开放全部监控，或选择配置，如 prometheus, health
management.endpoints.web.exposure.include=*
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;启动完成后通过 &lt;code&gt;{host}:{port}/actuator/prometheus&lt;/code&gt; 可以查看暴露的度量指标&lt;/p&gt;
&lt;h2 id=&#34;监控中心&#34;&gt;监控中心&lt;/h2&gt;
&lt;p&gt;通过 &lt;a href=&#34;https://prometheus.io/docs/prometheus/latest/getting_started/&#34;&gt;https://prometheus.io/docs/prometheus/latest/getting_started/&lt;/a&gt; 下载合适的 Prometheus 版本&lt;/p&gt;
&lt;p&gt;解压后修改 &lt;code&gt;prometheus.yml&lt;/code&gt; 配置，增加目标应用的监控配置&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;...

scrape_configs:
  - job_name: &#39;targetProject&#39;
    scrape_interval: 5s
    metrics_path: &#39;/actuator/prometheus&#39;
    static_configs:
    - targets: [&#39;127.0.0.1:8080&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;修改后通过命令 &lt;code&gt;./prometheus -config.file=prometheus.yml&lt;/code&gt; 启动 Prometheus，并进入管理页面 http://127.0.0.1:9090 检查 Endpoint 状态。&lt;/p&gt;
&lt;h2 id=&#34;grafana-数据可视化&#34;&gt;Grafana 数据可视化&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;通过 &lt;a href=&#34;http://docs.grafana.org/installation/&#34;&gt;http://docs.grafana.org/installation/&lt;/a&gt; 选择合适的版本安装&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;访问 http://127.0.0.1:3000 ，默认账号 admin/admin&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;添加 Prometheus 数据源，进入 &lt;code&gt;Configuration/DataSources&lt;/code&gt; 配置监控中心地址。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;导入 Dashboard，对数据的采集展示配置可通过 &lt;a href=&#34;https://grafana.com/dashboards&#34;&gt;https://grafana.com/dashboards&lt;/a&gt; 获取已有配置，填写 dashboardId 导入 (如 8919/4701)。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;也可通过 &lt;code&gt;Add panel&lt;/code&gt; 增加监控面板，再选择 &lt;code&gt;Add Query&lt;/code&gt; 设置监控主体，监控语句可通过 http://127.0.0.1:9090/graph 获取。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
